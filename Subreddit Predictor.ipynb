{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b84ca35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ec9bb54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>because she s the worst</td>\n",
       "      <td>d02u69l</td>\n",
       "      <td>anime</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>1455683054.0</td>\n",
       "      <td>Redire77</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>352.0</td>\n",
       "      <td>14017.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>i am referring to  this   http  iimgurcom5sryl...</td>\n",
       "      <td>466ijy</td>\n",
       "      <td>anime</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>1455682823.0</td>\n",
       "      <td>shiba_arata</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>cheating but zoldycks must have a great time a...</td>\n",
       "      <td>d02g879</td>\n",
       "      <td>anime</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>1455661236.0</td>\n",
       "      <td>ShaKing807</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1308.0</td>\n",
       "      <td>62021.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>kurosaki ichigo    http  images5fanpopcomimag...</td>\n",
       "      <td>d02v88z</td>\n",
       "      <td>anime</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>1455684994.0</td>\n",
       "      <td>Tf2idlingftw</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4156.0</td>\n",
       "      <td>1021.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0    0                                                  1  \\\n",
       "0           0  NaN                                                  0   \n",
       "1           1  0.0                           because she s the worst    \n",
       "2           2  1.0  i am referring to  this   http  iimgurcom5sryl...   \n",
       "3           3  2.0  cheating but zoldycks must have a great time a...   \n",
       "4           4  3.0   kurosaki ichigo    http  images5fanpopcomimag...   \n",
       "\n",
       "         2      3              4             5             6    7    8  \\\n",
       "0        1      2              3           4.0             5  6.0  7.0   \n",
       "1  d02u69l  anime  entertainment  1455683054.0      Redire77  7.0  0.0   \n",
       "2   466ijy  anime  entertainment  1455682823.0   shiba_arata  0.0  0.0   \n",
       "3  d02g879  anime  entertainment  1455661236.0    ShaKing807  6.0  0.0   \n",
       "4  d02v88z  anime  entertainment  1455684994.0  Tf2idlingftw  2.0  0.0   \n",
       "\n",
       "        9       10    11  \n",
       "0     8.0      9.0  10.0  \n",
       "1   352.0  14017.0   0.0  \n",
       "2     1.0     20.0   0.0  \n",
       "3  1308.0  62021.0   1.0  \n",
       "4  4156.0   1021.0   0.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing data from a public github repo containing reddit posts from 50 subreddits\n",
    "base_url = 'https://raw.githubusercontent.com/linanqiu/reddit-dataset/master/'\n",
    "\n",
    "file_names = [\n",
    "    'entertainment_anime.csv',\n",
    "    'entertainment_comicbooks.csv',\n",
    "    'entertainment_harrypotter.csv',\n",
    "    'entertainment_movies.csv',\n",
    "    'entertainment_music.csv',\n",
    "    'entertainment_starwars.csv',\n",
    "    'gaming_dota2.csv',\n",
    "    'gaming_gaming.csv',\n",
    "    'gaming_leagueoflegends.csv',\n",
    "    'gaming_minecraft.csv',\n",
    "    'gaming_pokemon.csv',\n",
    "    'gaming_skyrim.csv',\n",
    "    'gaming_starcraft.csv',\n",
    "    'gaming_tf2.csv',\n",
    "    'humor_adviceanimals.csv',\n",
    "    'humor_circlejerk.csv',\n",
    "    'humor_facepalm.csv',\n",
    "    'humor_funny.csv',\n",
    "    'humor_imgoingtohellforthis.csv',\n",
    "    'humor_jokes.csv',\n",
    "    'learning_askhistorians.csv',\n",
    "    'learning_askscience.csv',\n",
    "    'learning_explainlikeimfive.csv',\n",
    "    'learning_science.csv',\n",
    "    'learning_space.csv',\n",
    "    'learning_todayilearned.csv',\n",
    "    'learning_youshouldknow.csv',\n",
    "    'lifestyle_drunk.csv',\n",
    "    'lifestyle_food.csv',\n",
    "    'lifestyle_frugal.csv',\n",
    "    'lifestyle_guns.csv',\n",
    "    'lifestyle_lifehacks.csv',\n",
    "    'lifestyle_motorcycles.csv',\n",
    "    'lifestyle_progresspics.csv',\n",
    "    'lifestyle_sex.csv',\n",
    "    'news_conservative.csv',\n",
    "    'news_conspiracy.csv',\n",
    "    'news_libertarian.csv',\n",
    "    'news_news.csv',\n",
    "    'news_offbeat.csv',\n",
    "    'news_politics.csv',\n",
    "    'news_truereddit.csv',\n",
    "    'news_worldnews.csv',\n",
    "    'television_breakingbad.csv',\n",
    "    'television_community.csv',\n",
    "    'television_doctorwho.csv',\n",
    "    'television_gameofthrones.csv',\n",
    "    'television_himym.csv',\n",
    "    'television_mylittlepony.csv',\n",
    "    'television_startrek.csv',\n",
    "    'television_thewalkingdead.csv',\n",
    "]\n",
    "\n",
    "dataframes = []\n",
    "for file_name in file_names:\n",
    "    # Combine the base URL with the file name\n",
    "    file_url = f'{base_url}{file_name}'\n",
    "    df = pd.read_csv(file_url)\n",
    "    dataframes.append(df)\n",
    "\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dd18a7ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>because she s the worst</td>\n",
       "      <td>anime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i am referring to  this   http  iimgurcom5sryl...</td>\n",
       "      <td>anime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cheating but zoldycks must have a great time a...</td>\n",
       "      <td>anime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kurosaki ichigo    http  images5fanpopcomimag...</td>\n",
       "      <td>anime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>there are a shit ton of koutarous  but the pre...</td>\n",
       "      <td>anime</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text subreddit\n",
       "1                           because she s the worst      anime\n",
       "2  i am referring to  this   http  iimgurcom5sryl...     anime\n",
       "3  cheating but zoldycks must have a great time a...     anime\n",
       "4   kurosaki ichigo    http  images5fanpopcomimag...     anime\n",
       "5  there are a shit ton of koutarous  but the pre...     anime"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restructing the dataframe to get ready for preprocessing\n",
    "df = combined_df[['1','3']].astype(str)\n",
    "df.columns = ['text', 'subreddit']\n",
    "df = df.iloc[1:, :]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0131295e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning the text getting rid of special characters and numbers\n",
    "def clean_text(text):\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "df['text'] = df['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e8a93ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing common/insignificant english words\n",
    "df['text'] = df['text'].str.lower()\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not in stop_words])\n",
    "\n",
    "df['text'] = df['text'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "28146e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming and getting rid of short tokens\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "\n",
    "def remove_short_tokens(text):\n",
    "    return \" \".join([word for word in text.split() if len(word) > 2])\n",
    "\n",
    "df['text'] = df['text'].apply(lemmatize_text)\n",
    "df['text'] = df['text'].apply(remove_short_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b801b8aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>worst</td>\n",
       "      <td>anime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>referring http iimgurcom srylmijpg deeper mean...</td>\n",
       "      <td>anime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cheating zoldycks must great time thanksgiving</td>\n",
       "      <td>anime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kurosaki ichigo http image fanpopcomimagephoto...</td>\n",
       "      <td>anime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>shit ton koutarous presence one http smediacac...</td>\n",
       "      <td>anime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2663638</th>\n",
       "      <td>spm</td>\n",
       "      <td>television</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2663642</th>\n",
       "      <td>evj</td>\n",
       "      <td>television</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2663646</th>\n",
       "      <td>ouhb</td>\n",
       "      <td>television</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2663648</th>\n",
       "      <td>stxn</td>\n",
       "      <td>television</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2663649</th>\n",
       "      <td>vzrc</td>\n",
       "      <td>television</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1584199 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text   subreddit\n",
       "1                                                    worst       anime\n",
       "2        referring http iimgurcom srylmijpg deeper mean...       anime\n",
       "3           cheating zoldycks must great time thanksgiving       anime\n",
       "4        kurosaki ichigo http image fanpopcomimagephoto...       anime\n",
       "5        shit ton koutarous presence one http smediacac...       anime\n",
       "...                                                    ...         ...\n",
       "2663638                                                spm  television\n",
       "2663642                                                evj  television\n",
       "2663646                                               ouhb  television\n",
       "2663648                                               stxn  television\n",
       "2663649                                               vzrc  television\n",
       "\n",
       "[1584199 rows x 2 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace empty strings with NaN\n",
    "df.replace('', pd.NA, inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3cc27c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spltting data into train/test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['subreddit'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7c06a203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting text data to numerical data using TF-IDF vectorization\n",
    "vectorizer = TfidfVectorizer(max_features=10000)\n",
    "X_train_vect = vectorizer.fit_transform(X_train)\n",
    "X_test_vect = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0465cea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 20, 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "# further parameter tuning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [None, 10, 20, 30]\n",
    "}\n",
    "\n",
    "CV_clf = GridSearchCV(estimator=clf, param_grid=param_grid, cv=5)\n",
    "CV_clf.fit(X_train_vect, y_train)\n",
    "print(CV_clf.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d0752ba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(n_estimators=10, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(n_estimators=10, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(n_estimators=10, random_state=42)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training a random forest classifier\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=20, random_state=42)\n",
    "clf.fit(X_train_vect, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2aa4d022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making predictions based on classifier\n",
    "y_pred = clf.predict(X_test_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "363fd49d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.89456268147961113\n"
     ]
    }
   ],
   "source": [
    "# getting accuracy of my classifier\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c5b7eeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for applying the classifier to new text posts\n",
    "def predict_subreddit(text):\n",
    "    text = clean_text(text)  # Clean the new text\n",
    "    text_vect = vectorizer.transform([text])  # Vectorize the new text\n",
    "    prediction = clf.predict(text_vect)  # Predict the subreddit\n",
    "    return prediction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3a6f4e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(predict_subreddit(\"Here's a new Reddit post text to classify\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
